{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143120ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import dateparser\n",
    "from tqdm import tqdm\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c290d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('headless')    \n",
    "service = Service('./chromedriver')\n",
    "driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee402f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterfaxParser:\n",
    "    \n",
    "    def __init__(self, driver: webdriver, dates=None):\n",
    "        self.driver = driver\n",
    "        self.dates = dates\n",
    "        \n",
    "    def create_dates(self, start_date=datetime.date(2019,1,1), end_date=datetime.date.today()):\n",
    "        dates = []\n",
    "        # тесты на дебила :)\n",
    "        if start_date > end_date:\n",
    "            start_date, end_date = end_date, start_date\n",
    "            \n",
    "        if end_date > datetime.date.today():\n",
    "            end_date = datetime.date.today()\n",
    "            \n",
    "        if start_date < datetime.date(2000,1,1):\n",
    "            start_date = datetime.date(2000,1,1)\n",
    "            \n",
    "        # формируем даты\n",
    "        dates = pd.date_range(start=start_date, end=end_date, periods=len(range(start_date.year, end_date.year))*2)\n",
    "        dates = dates.strftime('%d.%m.%Y').tolist()\n",
    "#         start_day = start_date.strftime('%d')\n",
    "#         for year in range(start_date.year, end_date.year+1):\n",
    "#             dates.append(start_day + str(+str(year)) # например ['03.07.2021, 03.07.2022'], если стартовая дата 03.07.2021\n",
    "            \n",
    "        # если наша конечная дата не вошла в список\n",
    "        if pd.to_datetime(dates[-1], dayfirst=True).date() < end_date:\n",
    "            dates.append(end_date.strftime('%d.%m.%Y'))\n",
    "        # или если последний элемент списка превышает нашу конечную дату\n",
    "        elif pd.to_datetime(dates[-1], dayfirst=True).date() > end_date:\n",
    "            dates[-1] = end_date.strftime('%d.%m.%Y')\n",
    "\n",
    "        self.dates = dates\n",
    "        return dates\n",
    "    \n",
    "    \n",
    "    def __get_page_content(self, url, sleep_time=1) -> BeautifulSoup:\n",
    "        '''Открывает ссылку, парсит html'''\n",
    "        while True:\n",
    "            try:\n",
    "                self.driver.get(url) # открываем ссылку\n",
    "                time.sleep(sleep_time) # даем прогрузиться\n",
    "                break\n",
    "            except TimeoutException:\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "        page_source = self.driver.page_source # берем страницу\n",
    "        soup = BeautifulSoup(page_source, 'html5lib') # парсим\n",
    "\n",
    "        return soup\n",
    "    \n",
    "    \n",
    "    def __get_article(self, url):\n",
    "        soup = self.__get_page_content(url, 1)\n",
    "        \n",
    "        try:\n",
    "            time_div = soup.find('div', class_=\"d-flex align-items-sm-start align-items-center mb-20 flex-sm-column flex-md-row\")\n",
    "            text_dic = soup.find('div', class_='editor-content')\n",
    "            ps = text_dic.find_all('p')\n",
    "        except Exception as e:\n",
    "            print(e.__class__)\n",
    "            print(url)\n",
    "            print(soup)\n",
    "        \n",
    "        date = time_div.find_all('meta')[-1]['content'].split('T')[0]\n",
    "        text = ' '.join([p.text.strip('\\t') for p in ps]).strip()\n",
    "        \n",
    "        return date, text\n",
    "    \n",
    "    \n",
    "    def __parse_page(self, soup: BeautifulSoup, news, start, end) -> list:\n",
    "        \n",
    "        ul = soup.find('ul', class_='list-unstyled lenta-all-news')\n",
    "        news = ul.find_all('a', class_='d-block mb-0')\n",
    "        for i in tqdm(range(len(news))):\n",
    "            item = news[i]\n",
    "            cur_news = {}\n",
    "            cur_news['url'] = item['href']\n",
    "            cur_news['title'] = item.text\n",
    "            cur_news['date'], cur_news['text'] = self.__get_article(cur_news['url'])\n",
    "            news.append(cur_news)\n",
    "\n",
    "    \n",
    "    \n",
    "    def download_news(self, news=[]) -> list:\n",
    "        \n",
    "        if not self.dates:\n",
    "            self.create_dates()\n",
    "        \n",
    "        for i in range(len(self.dates)-1):\n",
    "            start, end = self.dates[i], self.dates[i+1]\n",
    "            url = f'https://www.interfax-russia.ru/ural/news?from={start}&to={end}&per-page=20000'\n",
    "            soup = self.__get_page_content(url, 5)\n",
    "            self.__parse_page(soup=soup, news=news, start=start, end=end)\n",
    "            with open(f'./interfax_news/interfax_ural_{start}_{end}', 'wb') as f:\n",
    "                pickle.dump(news, f)\n",
    "            time.sleep(20)\n",
    "            print(f'Updated news from {start} up to {end}!')\n",
    "            \n",
    "        return news       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb6efe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01.01.2009',\n",
       " '16.07.2009',\n",
       " '29.01.2010',\n",
       " '14.08.2010',\n",
       " '27.02.2011',\n",
       " '12.09.2011',\n",
       " '27.03.2012',\n",
       " '10.10.2012',\n",
       " '25.04.2013',\n",
       " '08.11.2013',\n",
       " '24.05.2014',\n",
       " '07.12.2014',\n",
       " '22.06.2015',\n",
       " '04.01.2016',\n",
       " '19.07.2016',\n",
       " '01.02.2017',\n",
       " '17.08.2017',\n",
       " '02.03.2018',\n",
       " '15.09.2018',\n",
       " '31.03.2019',\n",
       " '14.10.2019',\n",
       " '28.04.2020',\n",
       " '11.11.2020',\n",
       " '27.05.2021',\n",
       " '10.12.2021',\n",
       " '25.06.2022']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = InterfaxParser(driver)\n",
    "parser.create_dates(start_date=datetime.date(2009,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec59cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39de65bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████▋                                            | 2526/6286 [1:08:55<1:43:47,  1.66s/it]"
     ]
    }
   ],
   "source": [
    "dates = parser.download_news(news=news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355df03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
